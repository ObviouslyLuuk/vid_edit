{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook for testing the newest segment anything model by Facebook AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "from PIL import ImageOps\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sam2.sam2_video_predictor import SAM2VideoPredictor\n",
    "from sam_interactive_init import show_box, show_mask, show_points, get_frame_names\n",
    "from util import extract_frames\n",
    "\n",
    "# See if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # Use bfloat16 for faster inference\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract frames from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"../..\"\n",
    "VIDEO_NAME = \"back_test\"\n",
    "\n",
    "# Path to jpegs\n",
    "video_path = os.path.join(ROOT_DIR, VIDEO_NAME, VIDEO_NAME+\".mp4\")\n",
    "source_dir = os.path.join(ROOT_DIR, VIDEO_NAME, \"frames\")       # extracted frames\n",
    "FPS = 15\n",
    "extract_frames(video_path, source_dir, FPS, ext=\"jpg\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = os.path.join(ROOT_DIR, VIDEO_NAME, \"crop_frames\")   # resized and cropped frames\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "RESIZE = (144, 312)\n",
    "# CROP = (0, 110, 144, 280)\n",
    "CROP = (0, 130, 144, 290)\n",
    "\n",
    "# Show the first frame\n",
    "fig, ax = plt.subplots(1, 3, figsize=(5, 5))\n",
    "frame = Image.open(os.path.join(source_dir, \"0001.jpg\"))\n",
    "ax[0].imshow(frame)\n",
    "frame = frame.resize(RESIZE)\n",
    "ax[1].imshow(frame)\n",
    "frame = frame.crop(CROP)\n",
    "ax[2].imshow(frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess all frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(image, crop=CROP, resize=RESIZE):\n",
    "    # w, h = image.size\n",
    "    # image = image.resize((w // 2, h // 2))\n",
    "    image = image.resize(resize)\n",
    "    image = image.crop(crop)\n",
    "    # Apply slight gaussian blur\n",
    "    # image = image.filter(ImageFilter.GaussianBlur(1))\n",
    "    return image\n",
    "\n",
    "def reverse_process_frame(image, size, crop=CROP, resize=RESIZE):\n",
    "    \"\"\"\n",
    "    Take a processed frame and reverse the process,\n",
    "    for example gets 144x170 image, pads to 144x312, then resizes to wxh\n",
    "    \"\"\"\n",
    "    # Zero-pad the image to reverse the crop\n",
    "    re_w, re_h = resize\n",
    "    padding = (crop[0], crop[1], re_w - crop[2], re_h - crop[3])\n",
    "    image = ImageOps.expand(image, padding)\n",
    "    # Resize the image back to original size\n",
    "    image = image.resize(size)\n",
    "    return image\n",
    "\n",
    "# Crop all frames and save to video_dir\n",
    "for i, frame in tqdm(enumerate(os.listdir(source_dir)), total=len(os.listdir(source_dir))):\n",
    "    image = Image.open(os.path.join(source_dir, frame))\n",
    "    image = process_frame(image)\n",
    "    image.save(os.path.join(video_dir, frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan all the JPEG frame names in this directory\n",
    "frame_names = get_frame_names(video_dir)\n",
    "\n",
    "# take a look the first video frame\n",
    "frame_idx = 0\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor = SAM2VideoPredictor.from_pretrained(\"facebook/sam2-hiera-large\")\n",
    "# predictor = SAM2VideoPredictor.from_pretrained(\"facebook/sam2-hiera-base-plus\")\n",
    "# predictor = SAM2VideoPredictor.from_pretrained(\"facebook/sam2-hiera-small\")\n",
    "predictor = SAM2VideoPredictor.from_pretrained(\"facebook/sam2-hiera-tiny\")\n",
    "predictor = predictor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state = predictor.init_state(video_path=video_dir,\n",
    "    offload_video_to_cpu=True,\n",
    "    offload_state_to_cpu=True,\n",
    "    async_loading_frames=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run whenever we want to reset the tracking\n",
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Interactive Matplotlib Segmentation Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python sam_interactive_init.py --root_dir \"../..\" # This doesn't work for the interactive plot\n",
    "print(os.path.abspath(\"./sam_interactive_init.py\"))\n",
    "RAN_INTERACTIVE_SAM = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAN_INTERACTIVE_SAM:\n",
    "    prompts = torch.load(os.path.join(ROOT_DIR, VIDEO_NAME, \"sam_prompts.pth\"), weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RAN_INTERACTIVE_SAM:\n",
    "    ann_frame_idx = 0  # the frame index we interact with\n",
    "    ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "    # Let's add a positive click at (x, y) to get started\n",
    "    points = np.array([\n",
    "        [32, 20],\n",
    "        [32, 40],\n",
    "    ], dtype=np.float32)\n",
    "    # for labels, `1` means positive click and `0` means negative click\n",
    "    labels = np.array([\n",
    "        1,\n",
    "        0,\n",
    "    ], np.int32)\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=ann_frame_idx,\n",
    "        obj_id=ann_obj_id,\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "    )\n",
    "else:\n",
    "    ann_frame_idx = prompts[0][\"frame_idx\"]\n",
    "    ann_obj_id = prompts[0][\"obj_id\"]\n",
    "    points = prompts[0][\"points\"]\n",
    "    labels = prompts[0][\"labels\"]\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=ann_frame_idx,\n",
    "        obj_id=ann_obj_id,\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state, \n",
    "        start_frame_idx=None,\n",
    "        max_frame_num_to_track=None,\n",
    "        reverse=True):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "    if out_frame_idx > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 15\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    ann_frame_idx = 70  # further refine some details on this frame\n",
    "    ann_obj_id = 1  # give a unique id to the object we interact with (it can be any integers)\n",
    "\n",
    "    # show the segment before further refinement\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.title(f\"frame {ann_frame_idx} -- before refinement\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "    show_mask(video_segments[ann_frame_idx][ann_obj_id], plt.gca(), obj_id=ann_obj_id)\n",
    "\n",
    "    # Let's add a negative click on this frame at (x, y) to refine the segment\n",
    "    points = np.array([\n",
    "        [40, 55],\n",
    "        [42, 30],\n",
    "    ], dtype=np.float32)\n",
    "    # for labels, `1` means positive click and `0` means negative click\n",
    "    labels = np.array([\n",
    "        0,\n",
    "        1,\n",
    "    ], np.int32)\n",
    "    _, _, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=ann_frame_idx,\n",
    "        obj_id=ann_obj_id,\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "    # show the segment after the further refinement\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.title(f\"frame {ann_frame_idx} -- after refinement\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "    show_points(points, labels, plt.gca())\n",
    "    show_mask((out_mask_logits > 0.0).cpu().numpy(), plt.gca(), obj_id=ann_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dir = os.path.join(ROOT_DIR, VIDEO_NAME, \"mask_frames\")\n",
    "os.makedirs(mask_dir, exist_ok=True)\n",
    "\n",
    "# Get original size from source_dir\n",
    "image = Image.open(os.path.join(source_dir, frame_names[0]))\n",
    "original_size = image.size\n",
    "\n",
    "# save the segmentation results to disk\n",
    "for out_frame_idx, out_obj_ids in video_segments.items():\n",
    "    for out_obj_id, out_mask in out_obj_ids.items():\n",
    "        mask = (out_mask * 255).astype(np.uint8)\n",
    "        if mask.ndim == 2:  # Ensure the mask has 3 dimensions (H, W, 1)\n",
    "            mask = np.expand_dims(mask, axis=-1)\n",
    "        mask_image = Image.fromarray(mask.squeeze(), mode='L')\n",
    "        mask_image = reverse_process_frame(mask_image, original_size)\n",
    "        mask_image.save(os.path.join(mask_dir, f\"{out_frame_idx:04d}.png\"))\n",
    "\n",
    "# create a video from the segmentation results\n",
    "video_save_path = os.path.join(ROOT_DIR, VIDEO_NAME, \"mask.mp4\")\n",
    "fps = FPS\n",
    "os.system(f\"ffmpeg -y -r {fps} -f image2 -i {mask_dir}/%04d.png -vcodec libx264 -crf 25 -pix_fmt yuv420p {video_save_path}\")\n",
    "\n",
    "print(os.path.abspath(video_save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blur Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_blur_dir = os.path.join(ROOT_DIR, VIDEO_NAME, \"mask_blur_frames\")\n",
    "os.makedirs(mask_blur_dir, exist_ok=True)\n",
    "\n",
    "# Blur the mask in the original frames\n",
    "for out_frame_idx, out_obj_ids in video_segments.items():\n",
    "    image = Image.open(os.path.join(source_dir, frame_names[out_frame_idx]))\n",
    "    mask = out_obj_ids[ann_obj_id]\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    mask_image = Image.fromarray(mask.squeeze(), mode='L')\n",
    "    mask_image = reverse_process_frame(mask_image, original_size)\n",
    "    mask_image = mask_image.filter(ImageFilter.GaussianBlur(5))\n",
    "    # Blur the area covered by the mask in the original image\n",
    "    blur_image = image.copy()\n",
    "    # blur_image = image.filter(ImageFilter.GaussianBlur(20))\n",
    "\n",
    "    # Pixelate the area covered by the mask in the original image (do this by resizing the image back and forth)\n",
    "    pixelate_factor = 50\n",
    "    blur_image = blur_image.resize((original_size[0] // pixelate_factor, original_size[1] // pixelate_factor))\n",
    "    blur_image = blur_image.resize(original_size, Image.NEAREST)\n",
    "\n",
    "    # Paste the mask part of the blurred image on the original image\n",
    "    image.paste(blur_image, mask=mask_image)\n",
    "    image.save(os.path.join(mask_blur_dir, f\"{out_frame_idx:04d}.jpg\"))\n",
    "    # break\n",
    "\n",
    "# Plot first frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {0}\")\n",
    "plt.imshow(Image.open(os.path.join(mask_blur_dir, \"0000.jpg\")))\n",
    "plt.show()\n",
    "\n",
    "# create a video from the segmentation results\n",
    "video_save_path = os.path.join(ROOT_DIR, VIDEO_NAME, \"mask_blur.mp4\")\n",
    "fps = FPS\n",
    "os.system(f\"ffmpeg -y -r {fps} -i {mask_blur_dir}/%04d.jpg -vcodec libx264 -crf 25 -pix_fmt yuv420p {video_save_path}\")\n",
    "\n",
    "print(os.path.abspath(video_save_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
