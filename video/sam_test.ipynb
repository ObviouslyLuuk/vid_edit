{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook for testing the newest segment anything model by Facebook AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "from PIL import ImageOps\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sam2.sam2_video_predictor import SAM2VideoPredictor\n",
    "\n",
    "ROOT_DIR = \"../..\"\n",
    "VIDEO_NAME = \"face_test\"\n",
    "\n",
    "# Path to jpegs\n",
    "source_dir = os.path.join(ROOT_DIR, VIDEO_NAME, \"frames\")\n",
    "video_dir = os.path.join(ROOT_DIR, VIDEO_NAME, \"crop_frames\")\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "# Show the first frame\n",
    "frame = Image.open(os.path.join(source_dir, \"0001.jpg\"))\n",
    "plt.imshow(frame)\n",
    "plt.show()\n",
    "\n",
    "RESIZE = (144, 312)\n",
    "CROP = (0, 110, 144, 280)\n",
    "\n",
    "# See if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # Use bfloat16 for faster inference\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor = SAM2VideoPredictor.from_pretrained(\"facebook/sam2-hiera-large\")\n",
    "# predictor = SAM2VideoPredictor.from_pretrained(\"facebook/sam2-hiera-base-plus\")\n",
    "# predictor = SAM2VideoPredictor.from_pretrained(\"facebook/sam2-hiera-small\")\n",
    "predictor = SAM2VideoPredictor.from_pretrained(\"facebook/sam2-hiera-tiny\")\n",
    "predictor = predictor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(image, crop=CROP, resize=RESIZE):\n",
    "    # w, h = image.size\n",
    "    # image = image.resize((w // 2, h // 2))\n",
    "    image = image.resize(resize)\n",
    "    image = image.crop(crop)\n",
    "    # Apply slight gaussian blur\n",
    "    # image = image.filter(ImageFilter.GaussianBlur(1))\n",
    "    return image\n",
    "\n",
    "def reverse_process_frame(image, size, crop=CROP, resize=RESIZE):\n",
    "    \"\"\"\n",
    "    Take a processed frame and reverse the process,\n",
    "    for example gets 144x170 image, pads to 144x312, then resizes to wxh\n",
    "    \"\"\"\n",
    "    # Zero-pad the image to reverse the crop\n",
    "    re_w, re_h = resize\n",
    "    padding = (crop[0], crop[1], re_w - crop[2], re_h - crop[3])\n",
    "    image = ImageOps.expand(image, padding)\n",
    "    # Resize the image back to original size\n",
    "    image = image.resize(size)\n",
    "    return image\n",
    "\n",
    "# Crop all frames and save to video_dir\n",
    "for i, frame in tqdm(enumerate(os.listdir(source_dir))):\n",
    "    image = Image.open(os.path.join(source_dir, frame))\n",
    "    image = process_frame(image)\n",
    "    image.save(os.path.join(video_dir, frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan all the JPEG frame names in this directory\n",
    "frame_names = [\n",
    "    p for p in os.listdir(video_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "# take a look the first video frame\n",
    "frame_idx = 0\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python sam_interactive_init.py --root_dir \"../..\" # This doesn't work for the interactive plot\n",
    "print(os.path.abspath(\"./sam_interactive_init.py\"))\n",
    "RAN_INTERACTIVE_SAM = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAN_INTERACTIVE_SAM:\n",
    "    state = torch.load(os.path.join(ROOT_DIR, VIDEO_NAME, \"init_sam_state.pth\"), weights_only=False)\n",
    "    inference_state = state[\"inference_state\"]\n",
    "else:\n",
    "    inference_state = predictor.init_state(video_path=video_dir,\n",
    "        offload_video_to_cpu=True,\n",
    "        offload_state_to_cpu=True,\n",
    "        async_loading_frames=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RAN_INTERACTIVE_SAM:\n",
    "    # Run whenever we want to reset the tracking\n",
    "    predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RAN_INTERACTIVE_SAM:\n",
    "    ann_frame_idx = 0  # the frame index we interact with\n",
    "    ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "    # Let's add a positive click at (x, y) to get started\n",
    "    points = np.array([\n",
    "        [32, 20],\n",
    "        [32, 40],\n",
    "        [32, 45],\n",
    "        [20, 60], \n",
    "        [30, 70],\n",
    "        # [25, 44],\n",
    "        # [40, 44],\n",
    "    ], dtype=np.float32) * 2\n",
    "    # for labels, `1` means positive click and `0` means negative click\n",
    "    labels = np.array([\n",
    "        1,\n",
    "        1, \n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        # 1,\n",
    "        # 1,\n",
    "    ], np.int32)\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=ann_frame_idx,\n",
    "        obj_id=ann_obj_id,\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "    )\n",
    "else:\n",
    "    ann_frame_idx = state[\"frame_idx\"]\n",
    "    ann_obj_id = state[\"obj_id\"]\n",
    "    points = state[\"points\"]\n",
    "    labels = state[\"labels\"]\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=ann_frame_idx,\n",
    "        obj_id=ann_obj_id,\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 10\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    ann_frame_idx = 70  # further refine some details on this frame\n",
    "    ann_obj_id = 1  # give a unique id to the object we interact with (it can be any integers)\n",
    "\n",
    "    # show the segment before further refinement\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.title(f\"frame {ann_frame_idx} -- before refinement\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "    show_mask(video_segments[ann_frame_idx][ann_obj_id], plt.gca(), obj_id=ann_obj_id)\n",
    "\n",
    "    # Let's add a negative click on this frame at (x, y) to refine the segment\n",
    "    points = np.array([\n",
    "        [40, 55],\n",
    "        [42, 30],\n",
    "    ], dtype=np.float32)\n",
    "    # for labels, `1` means positive click and `0` means negative click\n",
    "    labels = np.array([\n",
    "        0,\n",
    "        1,\n",
    "    ], np.int32)\n",
    "    _, _, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=ann_frame_idx,\n",
    "        obj_id=ann_obj_id,\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "    # show the segment after the further refinement\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.title(f\"frame {ann_frame_idx} -- after refinement\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "    show_points(points, labels, plt.gca())\n",
    "    show_mask((out_mask_logits > 0.0).cpu().numpy(), plt.gca(), obj_id=ann_obj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dir = os.path.join(ROOT_DIR, \"face_test\", \"mask_frames\")\n",
    "os.makedirs(mask_dir, exist_ok=True)\n",
    "\n",
    "# Get original size from source_dir\n",
    "image = Image.open(os.path.join(source_dir, frame_names[0]))\n",
    "original_size = image.size\n",
    "\n",
    "# save the segmentation results to disk\n",
    "for out_frame_idx, out_obj_ids in video_segments.items():\n",
    "    for out_obj_id, out_mask in out_obj_ids.items():\n",
    "        mask = (out_mask * 255).astype(np.uint8)\n",
    "        if mask.ndim == 2:  # Ensure the mask has 3 dimensions (H, W, 1)\n",
    "            mask = np.expand_dims(mask, axis=-1)\n",
    "        mask_image = Image.fromarray(mask.squeeze(), mode='L')\n",
    "        mask_image = reverse_process_frame(mask_image, original_size)\n",
    "        mask_image.save(os.path.join(mask_dir, f\"{out_frame_idx:04d}.png\"))\n",
    "        break\n",
    "\n",
    "# create a video from the segmentation results\n",
    "video_save_path = os.path.join(ROOT_DIR, \"face_test\", \"mask.mp4\")\n",
    "fps = 15\n",
    "os.system(f\"ffmpeg -y -r {fps} -f image2 -i {mask_dir}/%04d.png -vcodec libx264 -crf 25 -pix_fmt yuv420p {video_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_blur_dir = os.path.join(ROOT_DIR, \"face_test\", \"mask_blur_frames\")\n",
    "os.makedirs(mask_blur_dir, exist_ok=True)\n",
    "\n",
    "# Blur the mask in the original frames\n",
    "for out_frame_idx, out_obj_ids in video_segments.items():\n",
    "    image = Image.open(os.path.join(source_dir, frame_names[out_frame_idx]))\n",
    "    mask = out_obj_ids[ann_obj_id]\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    mask_image = Image.fromarray(mask.squeeze(), mode='L')\n",
    "    mask_image = reverse_process_frame(mask_image, original_size)\n",
    "    mask_image = mask_image.filter(ImageFilter.GaussianBlur(5))\n",
    "    # Blur the area covered by the mask in the original image\n",
    "    blur_image = image.copy()\n",
    "    # blur_image = image.filter(ImageFilter.GaussianBlur(20))\n",
    "\n",
    "    # Pixelate the area covered by the mask in the original image (do this by resizing the image back and forth)\n",
    "    pixelate_factor = 50\n",
    "    blur_image = blur_image.resize((original_size[0] // pixelate_factor, original_size[1] // pixelate_factor))\n",
    "    blur_image = blur_image.resize(original_size, Image.NEAREST)\n",
    "\n",
    "    # Paste the mask part of the blurred image on the original image\n",
    "    image.paste(blur_image, mask=mask_image)\n",
    "    image.save(os.path.join(mask_blur_dir, f\"{out_frame_idx:04d}.jpg\"))\n",
    "    # break\n",
    "\n",
    "# Plot first frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {0}\")\n",
    "plt.imshow(Image.open(os.path.join(mask_blur_dir, \"0000.jpg\")))\n",
    "plt.show()\n",
    "\n",
    "# create a video from the segmentation results\n",
    "video_save_path = os.path.join(ROOT_DIR, \"face_test\", \"mask_blur.mp4\")\n",
    "fps = 15\n",
    "os.system(f\"ffmpeg -y -r {fps} -i {mask_blur_dir}/%04d.jpg -vcodec libx264 -crf 25 -pix_fmt yuv420p {video_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
